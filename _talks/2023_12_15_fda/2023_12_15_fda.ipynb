{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Statistical methods in information theory\"\n",
        "subtitle: \"and some clinical applications\"\n",
        "author: Soumik Purkayastha\n",
        "institute: Department of Biostatistics, University of Michigan\n",
        "bibliography: references.bib\n",
        "date: 2023-12-15\n",
        "format:\n",
        "  revealjs:\n",
        "    width: 1250\n",
        "    height: 1000\n",
        "    margin: 0.04\n",
        "    minScale: 0.2\n",
        "    maxScale: 2.0\n",
        "    aspectratio: 169\n",
        "    theme: metropolis.scss\n",
        "    toc: true\n",
        "    toc-depth: 1\n",
        "    chalkboard: false\n",
        "    slideNumber: true\n",
        "    menu: true\n",
        "---"
      ],
      "id": "08614748"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## About me\n",
        "\n",
        "# Information theory\n",
        "\n",
        "## Introduction\n",
        "\n",
        "-   Information theory (@shannon1948mathematical) is a branch of applied mathematics and electrical engineering that involves quantifying information.\n",
        "-   Original scope was to quantify namely data compression and error correction over a noisy channel.\n",
        "-   Now a fundamental concept in various fields, including *biostatistics*!\n",
        "    -   Evaluating diagnostic test performance through **mutual information** (@purkayastha2023fastmi).\n",
        "    -   Causal discovery for drug development through **entropy** (@purkayastha2023generative).\n",
        "\n",
        "## Some key information theoretic measures\n",
        "\n",
        "\n",
        "Consider random variables $X$ and $Y$ with marginal probability functions $p_X$ and $p_Y$ with joint probability function $p_{XY}$.\n",
        "\n",
        "::: columns\n",
        "\n",
        "::: {.column width=\"54%\"}\n",
        "::: {.callout-note title=\"Mutual information\" icon=\"false\"}\n",
        "\\begin{equation} MI(X, Y) := \\sum_{x, y} p_{XY}(x, y) \\log \\left(p_{XY}(x, y)/ \\left\\{ p_X(x)p_Y(y) \\right\\} \\right).\\end{equation}\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "::: {.callout-note title=\"Entropy\" icon=\"false\"}\n",
        "\\begin{equation}\n",
        "H(X) := \\sum_{x} - p_{X}(x) \\log \\left( p_X(x) \\right).\n",
        "\\end{equation}\n",
        ":::\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.callout-tip title=\"Properties\" icon=\"false\"}\n",
        "-   $MI$ is symmetric and always non-negative.\n",
        "\n",
        "-   $MI = 0$ if and only if $X$ and $Y$ are independent; $MI$ as a measure of stochastic dependence.\n",
        "\n",
        "-   $MI$ measures the quantity of information exchanged between the two random variables.; if $MI = 0$ then $X$ and $Y$ do not exchange information.\n",
        "\n",
        "-   $H(X)$ measures the average uncertainty or randomness in $P_X$.\n",
        "\n",
        "-   Intuitively $H(X)$ is linked to variability and $MI$ is linked to association (or correlation).\n",
        ":::\n",
        "\n",
        "\n",
        "# Application I: evaluating diagnostic test performance\n",
        "\n",
        "## Translating information theory to medical diagnostics\n",
        "\n",
        "-   We have a state of disease described by $D$; it takes its value in the set $$S_D =\\left\\{d_1,\\ldots,d_K\\right\\}.$$\n",
        "\n",
        "-   We have $R$ describing the outcome of a diagnostic test, taking its value in the set $$S_R =\\left\\{r_1,\\ldots,r_M\\right\\}.$$\n",
        "\n",
        "::: {.callout-important title=\"Key idea\" icon=\"false\"}\n",
        "-   Diagnostic tests are evaluated on sensitivity (SE) and specificity (SP).\n",
        "-   $MI$ serves as single statistical measure that can summarize the global quality of a dichotomous diagnostic test while incorporating information on SE and SP.\n",
        ":::\n",
        "\n",
        "::: {.callout-caution title=\"Simplifying assumptions\" icon=\"false\"}\n",
        "-   There are only two mutually exclusive disease states $D$: either present $(D = 1)$ or absent $(D = 0)$.\n",
        "-   There are only two mutually exclusive diagnostic outcomes $R$: either positive $(R = 1)$ or negative $(R = 0)$.\n",
        ":::\n",
        "\n",
        "## Measuring the quality of a diagnostic test\n",
        "\n",
        "::: columns\n",
        "\n",
        "::: {.column width=\"47%\"}\n",
        "<center>\n",
        "![SE and SP describe test performance.](figure2.jpeg)\n",
        "</center>\n",
        ":::\n",
        "\n",
        "::: {.column width=\"47%\"}\n",
        "<center>\n",
        "![Summarizing information flow in medical diagnostics.](figure1.png){fig-align=\"center\"}\n",
        "</center>\n",
        ":::\n",
        ":::\n",
        "\n",
        ". . . \n",
        "\n",
        "::: {.callout-important title=\"Key results\" icon=\"false\"}\n",
        "-  $MI$ between the diagnostic test $D$ and disease status $R$ depends on $SE, SP,$ and prevalence of $D$ (@casagrande2022fifty).\n",
        "-  This measure is subject to the prevalence, which is not always known (can get estimates from data, may be biased). \n",
        "-  For given SE and SP, we can evaluate the mutual information itself for any possible prevalence $p$, denoted by $MI_{SE, SP}(p)$. \n",
        " - To define a prevalence-independent metric, we consider the measure $MI(SE, SP) := \\int_{[0, 1]} MI_{SE, SP}(p) dp.$\n",
        ":::\n",
        "\n",
        "## Connection to ROC and AUC\n",
        "<center>\n",
        "![Given $SE$ and $SP$ we can plot $MI_{SE, SP}(p)$](fig3.jpeg){fig-align=\"center\"}\n",
        "</center>\n",
        "\n",
        "- The $MI_{SE, SP}(p)$ curve is analogous to the ROC curve. \n",
        "- $MI(SE, SP) := \\int_{[0, 1]} MI_{SE, SP}(p) dp$ serves as an AUC analog.\n",
        "-  Given SE and SP for three COVID-19 antibody tests, we compare $MI(SE, SP)$ and return a ranking of test efficacy.\n",
        "\n",
        "\n",
        "## $MI$-based evaluation of serology tests for COVID-19\n"
      ],
      "id": "d98d5fc7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Application II: causal discovery for drug development\n",
        "\n",
        "# Some side projects\n",
        "\n",
        "# References\n",
        "\n",
        "::: {#refs}\n",
        ":::"
      ],
      "id": "53b257cc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}